import torch
from torch import nn
import torch
import torchvision
from torch import nn
from torch.autograd import Variable
from torch.utils.data import DataLoader
from torchvision import transforms
from torchvision.utils import save_image
import torch.nn.functional as F
import os
import matplotlib.pyplot as plt
from utils import *

import timm
from timm.models.layers import DropPath, to_2tuple, trunc_normal_
import types
import math
from abc import ABCMeta, abstractmethod
# from mmcv.cnn import ConvModule
from pdb import set_trace as st

from kan import KANLinear, KAN
from torch.nn import init

import time
import math
from functools import partial
from typing import Optional, Callable

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.utils.checkpoint as checkpoint
from einops import rearrange, repeat
from timm.models.layers import DropPath, to_2tuple, trunc_normal_
try:
    from mamba_ssm.ops.selective_scan_interface import selective_scan_fn, selective_scan_ref
except:
    pass

# an alternative for mamba_ssm (in which causal_conv1d is needed)
try:
    from selective_scan import selective_scan_fn as selective_scan_fn_v1
    from selective_scan import selective_scan_ref as selective_scan_ref_v1
except:
    pass

__all__ = ['UKAN','UKANPP','KM_UNet','KMUNetPP']

class KANLayer(nn.Module):
    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0., no_kan=False):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        self.dim = in_features
        
        grid_size=5
        spline_order=3
        scale_noise=0.1
        scale_base=1.0
        scale_spline=1.0
        base_activation=torch.nn.SiLU
        grid_eps=0.02
        grid_range=[-1, 1]

        if not no_kan:
            self.fc1 = KANLinear(
                        in_features,
                        hidden_features,
                        grid_size=grid_size,
                        spline_order=spline_order,
                        scale_noise=scale_noise,
                        scale_base=scale_base,
                        scale_spline=scale_spline,
                        base_activation=base_activation,
                        grid_eps=grid_eps,
                        grid_range=grid_range,
                    )
            self.fc2 = KANLinear(
                        hidden_features,
                        out_features,
                        grid_size=grid_size,
                        spline_order=spline_order,
                        scale_noise=scale_noise,
                        scale_base=scale_base,
                        scale_spline=scale_spline,
                        base_activation=base_activation,
                        grid_eps=grid_eps,
                        grid_range=grid_range,
                    )
            self.fc3 = KANLinear(
                        hidden_features,
                        out_features,
                        grid_size=grid_size,
                        spline_order=spline_order,
                        scale_noise=scale_noise,
                        scale_base=scale_base,
                        scale_spline=scale_spline,
                        base_activation=base_activation,
                        grid_eps=grid_eps,
                        grid_range=grid_range,
                    )
            # # TODO   
            # self.fc4 = KANLinear(
            #             hidden_features,
            #             out_features,
            #             grid_size=grid_size,
            #             spline_order=spline_order,
            #             scale_noise=scale_noise,
            #             scale_base=scale_base,
            #             scale_spline=scale_spline,
            #             base_activation=base_activation,
            #             grid_eps=grid_eps,
            #             grid_range=grid_range,
            #         )   

        else:
            self.fc1 = nn.Linear(in_features, hidden_features)
            self.fc2 = nn.Linear(hidden_features, out_features)
            self.fc3 = nn.Linear(hidden_features, out_features)

        # TODO
        # self.fc1 = nn.Linear(in_features, hidden_features)


        self.dwconv_1 = DW_bn_relu(hidden_features)
        self.dwconv_2 = DW_bn_relu(hidden_features)
        self.dwconv_3 = DW_bn_relu(hidden_features)

        # # TODO
        # self.dwconv_4 = DW_bn_relu(hidden_features)
    
        self.drop = nn.Dropout(drop)

        self.apply(self._init_weights)

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)
        elif isinstance(m, nn.Conv2d):
            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
            fan_out //= m.groups
            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))
            if m.bias is not None:
                m.bias.data.zero_()
    

    def forward(self, x, H, W):
        # pdb.set_trace()
        B, N, C = x.shape

        x = self.fc1(x.reshape(B*N,C))
        x = x.reshape(B,N,C).contiguous()
        x = self.dwconv_1(x, H, W)
        x = self.fc2(x.reshape(B*N,C))
        x = x.reshape(B,N,C).contiguous()
        x = self.dwconv_2(x, H, W)
        x = self.fc3(x.reshape(B*N,C))
        x = x.reshape(B,N,C).contiguous()
        x = self.dwconv_3(x, H, W)

        # # TODO
        # x = x.reshape(B,N,C).contiguous()
        # x = self.dwconv_4(x, H, W)
    
        return x

class KANBlock(nn.Module):
    def __init__(self, dim, drop=0., drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, no_kan=False):
        super().__init__()

        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
        self.norm2 = norm_layer(dim)
        mlp_hidden_dim = int(dim)

        self.layer = KANLayer(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop, no_kan=no_kan)

        self.apply(self._init_weights)

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)
        elif isinstance(m, nn.Conv2d):
            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
            fan_out //= m.groups
            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))
            if m.bias is not None:
                m.bias.data.zero_()

    def forward(self, x, H, W):
        x = x + self.drop_path(self.layer(self.norm2(x), H, W))

        return x


class DWConv(nn.Module):
    def __init__(self, dim=768):
        super(DWConv, self).__init__()
        self.dwconv = nn.Conv2d(dim, dim, 3, 1, 1, bias=True, groups=dim)

    def forward(self, x, H, W):
        B, N, C = x.shape
        x = x.transpose(1, 2).view(B, C, H, W)
        x = self.dwconv(x)
        x = x.flatten(2).transpose(1, 2)

        return x

class DW_bn_relu(nn.Module):
    def __init__(self, dim=768):
        super(DW_bn_relu, self).__init__()
        self.dwconv = nn.Conv2d(dim, dim, 3, 1, 1, bias=True, groups=dim)
        self.bn = nn.BatchNorm2d(dim)
        self.relu = nn.ReLU()

    def forward(self, x, H, W):
        B, N, C = x.shape
        x = x.transpose(1, 2).view(B, C, H, W)
        x = self.dwconv(x)
        x = self.bn(x)
        x = self.relu(x)
        x = x.flatten(2).transpose(1, 2)

        return x

class PatchEmbed(nn.Module):
    """ Image to Patch Embedding
    """

    def __init__(self, img_size=224, patch_size=7, stride=4, in_chans=3, embed_dim=768):
        super().__init__()
        img_size = to_2tuple(img_size)
        patch_size = to_2tuple(patch_size)

        self.img_size = img_size
        self.patch_size = patch_size
        self.H, self.W = img_size[0] // patch_size[0], img_size[1] // patch_size[1]
        self.num_patches = self.H * self.W
        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=stride,
                              padding=(patch_size[0] // 2, patch_size[1] // 2))
        self.norm = nn.LayerNorm(embed_dim)

        self.apply(self._init_weights)

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)
        elif isinstance(m, nn.Conv2d):
            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
            fan_out //= m.groups
            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))
            if m.bias is not None:
                m.bias.data.zero_()

    def forward(self, x):
        x = self.proj(x)
        _, _, H, W = x.shape
        x = x.flatten(2).transpose(1, 2)
        x = self.norm(x)

        return x, H, W


class ConvLayer(nn.Module):
    def __init__(self, in_ch, out_ch):
        super(ConvLayer, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(in_ch, out_ch, 3, padding=1),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_ch, out_ch, 3, padding=1),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True)
        )

    def forward(self, input):
        return self.conv(input)

class D_ConvLayer(nn.Module):
    def __init__(self, in_ch, out_ch):
        super(D_ConvLayer, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(in_ch, in_ch, 3, padding=1),
            nn.BatchNorm2d(in_ch),
            nn.ReLU(inplace=True),
            nn.Conv2d(in_ch, out_ch, 3, padding=1),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True)
        )

    def forward(self, input):
        return self.conv(input)

# 添加cbam注意力机制（效果没变化）
class ChannelAttentionModule(nn.Module):
    def __init__(self, channel, ratio=16):
        super(ChannelAttentionModule, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.max_pool = nn.AdaptiveMaxPool2d(1)

        self.shared_MLP = nn.Sequential(
            nn.Conv2d(channel, channel // ratio, 1, bias=False),
            nn.ReLU(),
            nn.Conv2d(channel // ratio, channel, 1, bias=False)
        )
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avgout = self.shared_MLP(self.avg_pool(x))
        maxout = self.shared_MLP(self.max_pool(x))
        return self.sigmoid(avgout + maxout)

class SpatialAttentionModule(nn.Module):
    def __init__(self):
        super(SpatialAttentionModule, self).__init__()
        self.conv2d = nn.Conv2d(in_channels=2, out_channels=1, kernel_size=7, stride=1, padding=3)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avgout = torch.mean(x, dim=1, keepdim=True)
        maxout, _ = torch.max(x, dim=1, keepdim=True)
        out = torch.cat([avgout, maxout], dim=1)
        out = self.sigmoid(self.conv2d(out))
        return out

class CBAM(nn.Module):
    def __init__(self, channel):
        super(CBAM, self).__init__()
        self.channel_attention = ChannelAttentionModule(channel)
        self.spatial_attention = SpatialAttentionModule()

    def forward(self, x):
        out = self.channel_attention(x) * x
        out = self.spatial_attention(out) * out
        return out
# 添加注意力模块
import torch
from torch import nn

class EMA(nn.Module):
    def __init__(self, channels, factor=8):
        super(EMA, self).__init__()
        self.groups = factor
        assert channels // self.groups > 0
        self.softmax = nn.Softmax(-1)
        self.agp = nn.AdaptiveAvgPool2d((1, 1))
        self.pool_h = nn.AdaptiveAvgPool2d((None, 1))
        self.pool_w = nn.AdaptiveAvgPool2d((1, None))
        self.gn = nn.GroupNorm(channels // self.groups, channels // self.groups)
        self.conv1x1 = nn.Conv2d(channels // self.groups, channels // self.groups, kernel_size=1, stride=1, padding=0)
        self.conv3x3 = nn.Conv2d(channels // self.groups, channels // self.groups, kernel_size=3, stride=1, padding=1)

    def forward(self, x):
        b, c, h, w = x.size()
        group_x = x.reshape(b * self.groups, -1, h, w)  # b*g,c//g,h,w
        x_h = self.pool_h(group_x)
        x_w = self.pool_w(group_x).permute(0, 1, 3, 2)
        hw = self.conv1x1(torch.cat([x_h, x_w], dim=2))
        x_h, x_w = torch.split(hw, [h, w], dim=2)
        x1 = self.gn(group_x * x_h.sigmoid() * x_w.permute(0, 1, 3, 2).sigmoid())
        x2 = self.conv3x3(group_x)
        x11 = self.softmax(self.agp(x1).reshape(b * self.groups, -1, 1).permute(0, 2, 1))
        x12 = x2.reshape(b * self.groups, c // self.groups, -1)  # b*g, c//g, hw
        x21 = self.softmax(self.agp(x2).reshape(b * self.groups, -1, 1).permute(0, 2, 1))
        x22 = x1.reshape(b * self.groups, c // self.groups, -1)  # b*g, c//g, hw
        weights = (torch.matmul(x11, x12) + torch.matmul(x21, x22)).reshape(b * self.groups, 1, h, w)
        return (group_x * weights.sigmoid()).reshape(b, c, h, w)

    
# 添加ss2d块
class SS2D(nn.Module):
    def __init__(
        self,
        d_model,
        d_state=16,
        # d_state="auto", # 20240109
        d_conv=3,
        expand=2,
        dt_rank="auto",
        dt_min=0.001,
        dt_max=0.1,
        dt_init="random",
        dt_scale=1.0,
        dt_init_floor=1e-4,
        dropout=0.,
        conv_bias=True,
        bias=False,
        device=None,
        dtype=None,
        **kwargs,
    ):
        factory_kwargs = {"device": device, "dtype": dtype}
        super().__init__()
        self.d_model = d_model
        self.d_state = d_state
        # self.d_state = math.ceil(self.d_model / 6) if d_state == "auto" else d_model # 20240109
        self.d_conv = d_conv
        self.expand = expand
        self.d_inner = int(self.expand * self.d_model)
        self.dt_rank = math.ceil(self.d_model / 16) if dt_rank == "auto" else dt_rank

        self.in_proj = nn.Linear(self.d_model, self.d_inner * 2, bias=bias, **factory_kwargs)
        self.conv2d = nn.Conv2d(
            in_channels=self.d_inner,
            out_channels=self.d_inner,
            groups=self.d_inner,
            bias=conv_bias,
            kernel_size=d_conv,
            padding=(d_conv - 1) // 2,
            **factory_kwargs,
        )
        self.act = nn.SiLU()

        self.x_proj = (
            nn.Linear(self.d_inner, (self.dt_rank + self.d_state * 2), bias=False, **factory_kwargs), 
            nn.Linear(self.d_inner, (self.dt_rank + self.d_state * 2), bias=False, **factory_kwargs), 
            nn.Linear(self.d_inner, (self.dt_rank + self.d_state * 2), bias=False, **factory_kwargs), 
            nn.Linear(self.d_inner, (self.dt_rank + self.d_state * 2), bias=False, **factory_kwargs), 
        )
        self.x_proj_weight = nn.Parameter(torch.stack([t.weight for t in self.x_proj], dim=0)) # (K=4, N, inner)
        del self.x_proj

        self.dt_projs = (
            self.dt_init(self.dt_rank, self.d_inner, dt_scale, dt_init, dt_min, dt_max, dt_init_floor, **factory_kwargs),
            self.dt_init(self.dt_rank, self.d_inner, dt_scale, dt_init, dt_min, dt_max, dt_init_floor, **factory_kwargs),
            self.dt_init(self.dt_rank, self.d_inner, dt_scale, dt_init, dt_min, dt_max, dt_init_floor, **factory_kwargs),
            self.dt_init(self.dt_rank, self.d_inner, dt_scale, dt_init, dt_min, dt_max, dt_init_floor, **factory_kwargs),
        )
        self.dt_projs_weight = nn.Parameter(torch.stack([t.weight for t in self.dt_projs], dim=0)) # (K=4, inner, rank)
        self.dt_projs_bias = nn.Parameter(torch.stack([t.bias for t in self.dt_projs], dim=0)) # (K=4, inner)
        del self.dt_projs
        
        self.A_logs = self.A_log_init(self.d_state, self.d_inner, copies=4, merge=True) # (K=4, D, N)
        self.Ds = self.D_init(self.d_inner, copies=4, merge=True) # (K=4, D, N)

        # self.selective_scan = selective_scan_fn
        self.forward_core = self.forward_corev0

        self.out_norm = nn.LayerNorm(self.d_inner)
        self.out_proj = nn.Linear(self.d_inner, self.d_model, bias=bias, **factory_kwargs)
        self.dropout = nn.Dropout(dropout) if dropout > 0. else None

    @staticmethod
    def dt_init(dt_rank, d_inner, dt_scale=1.0, dt_init="random", dt_min=0.001, dt_max=0.1, dt_init_floor=1e-4, **factory_kwargs):
        dt_proj = nn.Linear(dt_rank, d_inner, bias=True, **factory_kwargs)

        # Initialize special dt projection to preserve variance at initialization
        dt_init_std = dt_rank**-0.5 * dt_scale
        if dt_init == "constant":
            nn.init.constant_(dt_proj.weight, dt_init_std)
        elif dt_init == "random":
            nn.init.uniform_(dt_proj.weight, -dt_init_std, dt_init_std)
        else:
            raise NotImplementedError

        # Initialize dt bias so that F.softplus(dt_bias) is between dt_min and dt_max
        dt = torch.exp(
            torch.rand(d_inner, **factory_kwargs) * (math.log(dt_max) - math.log(dt_min))
            + math.log(dt_min)
        ).clamp(min=dt_init_floor)
        # Inverse of softplus: https://github.com/pytorch/pytorch/issues/72759
        inv_dt = dt + torch.log(-torch.expm1(-dt))
        with torch.no_grad():
            dt_proj.bias.copy_(inv_dt)
        # Our initialization would set all Linear.bias to zero, need to mark this one as _no_reinit
        dt_proj.bias._no_reinit = True
        
        return dt_proj

    @staticmethod
    def A_log_init(d_state, d_inner, copies=1, device=None, merge=True):
        # S4D real initialization
        A = repeat(
            torch.arange(1, d_state + 1, dtype=torch.float32, device=device),
            "n -> d n",
            d=d_inner,
        ).contiguous()
        A_log = torch.log(A)  # Keep A_log in fp32
        if copies > 1:
            A_log = repeat(A_log, "d n -> r d n", r=copies)
            if merge:
                A_log = A_log.flatten(0, 1)
        A_log = nn.Parameter(A_log)
        A_log._no_weight_decay = True
        return A_log

    @staticmethod
    def D_init(d_inner, copies=1, device=None, merge=True):
        # D "skip" parameter
        D = torch.ones(d_inner, device=device)
        if copies > 1:
            D = repeat(D, "n1 -> r n1", r=copies)
            if merge:
                D = D.flatten(0, 1)
        D = nn.Parameter(D)  # Keep in fp32
        D._no_weight_decay = True
        return D

    def forward_corev0(self, x: torch.Tensor):
        self.selective_scan = selective_scan_fn
        
        B, C, H, W = x.shape
        L = H * W
        K = 4

        x_hwwh = torch.stack([x.view(B, -1, L), torch.transpose(x, dim0=2, dim1=3).contiguous().view(B, -1, L)], dim=1).view(B, 2, -1, L)
        xs = torch.cat([x_hwwh, torch.flip(x_hwwh, dims=[-1])], dim=1) # (b, k, d, l)

        x_dbl = torch.einsum("b k d l, k c d -> b k c l", xs.view(B, K, -1, L), self.x_proj_weight)
        # x_dbl = x_dbl + self.x_proj_bias.view(1, K, -1, 1)
        dts, Bs, Cs = torch.split(x_dbl, [self.dt_rank, self.d_state, self.d_state], dim=2)
        dts = torch.einsum("b k r l, k d r -> b k d l", dts.view(B, K, -1, L), self.dt_projs_weight)
        # dts = dts + self.dt_projs_bias.view(1, K, -1, 1)

        xs = xs.float().view(B, -1, L) # (b, k * d, l)
        dts = dts.contiguous().float().view(B, -1, L) # (b, k * d, l)
        Bs = Bs.float().view(B, K, -1, L) # (b, k, d_state, l)
        Cs = Cs.float().view(B, K, -1, L) # (b, k, d_state, l)
        Ds = self.Ds.float().view(-1) # (k * d)
        As = -torch.exp(self.A_logs.float()).view(-1, self.d_state)  # (k * d, d_state)
        dt_projs_bias = self.dt_projs_bias.float().view(-1) # (k * d)

        out_y = self.selective_scan(
            xs, dts, 
            As, Bs, Cs, Ds, z=None,
            delta_bias=dt_projs_bias,
            delta_softplus=True,
            return_last_state=False,
        ).view(B, K, -1, L)
        assert out_y.dtype == torch.float

        inv_y = torch.flip(out_y[:, 2:4], dims=[-1]).view(B, 2, -1, L)
        wh_y = torch.transpose(out_y[:, 1].view(B, -1, W, H), dim0=2, dim1=3).contiguous().view(B, -1, L)
        invwh_y = torch.transpose(inv_y[:, 1].view(B, -1, W, H), dim0=2, dim1=3).contiguous().view(B, -1, L)

        return out_y[:, 0], inv_y[:, 0], wh_y, invwh_y

    # an alternative to forward_corev1
    def forward_corev1(self, x: torch.Tensor):
        self.selective_scan = selective_scan_fn_v1

        B, C, H, W = x.shape
        L = H * W
        K = 4

        x_hwwh = torch.stack([x.view(B, -1, L), torch.transpose(x, dim0=2, dim1=3).contiguous().view(B, -1, L)], dim=1).view(B, 2, -1, L)
        xs = torch.cat([x_hwwh, torch.flip(x_hwwh, dims=[-1])], dim=1) # (b, k, d, l)

        x_dbl = torch.einsum("b k d l, k c d -> b k c l", xs.view(B, K, -1, L), self.x_proj_weight)
        # x_dbl = x_dbl + self.x_proj_bias.view(1, K, -1, 1)
        dts, Bs, Cs = torch.split(x_dbl, [self.dt_rank, self.d_state, self.d_state], dim=2)
        dts = torch.einsum("b k r l, k d r -> b k d l", dts.view(B, K, -1, L), self.dt_projs_weight)
        # dts = dts + self.dt_projs_bias.view(1, K, -1, 1)

        xs = xs.float().view(B, -1, L) # (b, k * d, l)
        dts = dts.contiguous().float().view(B, -1, L) # (b, k * d, l)
        Bs = Bs.float().view(B, K, -1, L) # (b, k, d_state, l)
        Cs = Cs.float().view(B, K, -1, L) # (b, k, d_state, l)
        Ds = self.Ds.float().view(-1) # (k * d)
        As = -torch.exp(self.A_logs.float()).view(-1, self.d_state)  # (k * d, d_state)
        dt_projs_bias = self.dt_projs_bias.float().view(-1) # (k * d)

        out_y = self.selective_scan(
            xs, dts, 
            As, Bs, Cs, Ds,
            delta_bias=dt_projs_bias,
            delta_softplus=True,
        ).view(B, K, -1, L)
        assert out_y.dtype == torch.float

        inv_y = torch.flip(out_y[:, 2:4], dims=[-1]).view(B, 2, -1, L)
        wh_y = torch.transpose(out_y[:, 1].view(B, -1, W, H), dim0=2, dim1=3).contiguous().view(B, -1, L)
        invwh_y = torch.transpose(inv_y[:, 1].view(B, -1, W, H), dim0=2, dim1=3).contiguous().view(B, -1, L)

        return out_y[:, 0], inv_y[:, 0], wh_y, invwh_y

    def forward(self, x: torch.Tensor, **kwargs):
        B, H, W, C = x.shape

        xz = self.in_proj(x)
        x, z = xz.chunk(2, dim=-1) # (b, h, w, d)

        x = x.permute(0, 3, 1, 2).contiguous()
        x = self.act(self.conv2d(x)) # (b, d, h, w)
        y1, y2, y3, y4 = self.forward_core(x)
        assert y1.dtype == torch.float32
        y = y1 + y2 + y3 + y4
        y = torch.transpose(y, dim0=1, dim1=2).contiguous().view(B, H, W, -1)
        y = self.out_norm(y)
        y = y * F.silu(z)
        out = self.out_proj(y)
        if self.dropout is not None:
            out = self.dropout(out)
        return out    

class UKAN(nn.Module):
    def __init__(self, num_classes, input_channels=3, deep_supervision=False, img_size=224, patch_size=16, in_chans=3, embed_dims=[256, 320, 512], no_kan=False,
    drop_rate=0., drop_path_rate=0., norm_layer=nn.LayerNorm, depths=[1, 1, 1], **kwargs):
        super().__init__()

        kan_input_dim = embed_dims[0]

        self.encoder1 = ConvLayer(3, kan_input_dim//8)  
        self.encoder2 = ConvLayer(kan_input_dim//8, kan_input_dim//4)  
        self.encoder3 = ConvLayer(kan_input_dim//4, kan_input_dim)

        self.norm3 = norm_layer(embed_dims[1])
        self.norm4 = norm_layer(embed_dims[2])

        self.dnorm3 = norm_layer(embed_dims[1])
        self.dnorm4 = norm_layer(embed_dims[0])

        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]

        self.block1 = nn.ModuleList([KANBlock(
            dim=embed_dims[1], 
            drop=drop_rate, drop_path=dpr[0], norm_layer=norm_layer
            )])

        self.block2 = nn.ModuleList([KANBlock(
            dim=embed_dims[2],
            drop=drop_rate, drop_path=dpr[1], norm_layer=norm_layer
            )])

        self.dblock1 = nn.ModuleList([KANBlock(
            dim=embed_dims[1], 
            drop=drop_rate, drop_path=dpr[0], norm_layer=norm_layer
            )])

        self.dblock2 = nn.ModuleList([KANBlock(
            dim=embed_dims[0], 
            drop=drop_rate, drop_path=dpr[1], norm_layer=norm_layer
            )])

        self.patch_embed3 = PatchEmbed(img_size=img_size // 4, patch_size=3, stride=2, in_chans=embed_dims[0], embed_dim=embed_dims[1])
        self.patch_embed4 = PatchEmbed(img_size=img_size // 8, patch_size=3, stride=2, in_chans=embed_dims[1], embed_dim=embed_dims[2])

        self.decoder1 = D_ConvLayer(embed_dims[2], embed_dims[1])  
        self.decoder2 = D_ConvLayer(embed_dims[1], embed_dims[0])  
        self.decoder3 = D_ConvLayer(embed_dims[0], embed_dims[0]//4) 
        self.decoder4 = D_ConvLayer(embed_dims[0]//4, embed_dims[0]//8)
        self.decoder5 = D_ConvLayer(embed_dims[0]//8, embed_dims[0]//8)

        self.final = nn.Conv2d(embed_dims[0]//8, num_classes, kernel_size=1)
        self.soft = nn.Softmax(dim =1)
        self.cbam = CBAM(channel=16)
        self.cbam1 = CBAM(channel=32)
        self.cbam2 = CBAM(channel=128)

    
# put shape: torch.Size([8, 3, 256, 256])
# After Stage 1 (encoder1) shape: torch.Size([8, 16, 128, 128])
# After Stage 2 (encoder2) shape: torch.Size([8, 32, 64, 64])
# After Stage 3 (encoder3) shape: torch.Size([8, 128, 32, 32])
# After Stage 4 (patch_embed3) shape: torch.Size([8, 256, 160]), H: 16, W: 16
# After norm3 and reshape shape: torch.Size([8, 160, 16, 16])
# After Bottleneck (patch_embed4) shape: torch.Size([8, 64, 256]), H: 8, W: 8
# After norm4 and reshape shape: torch.Size([8, 256, 8, 8])
# After decoder1 shape: torch.Size([8, 160, 16, 16])
# After add t4 shape: torch.Size([8, 160, 16, 16])
# After dblock1 shape: torch.Size([8, 256, 160])
# After dnorm3 and reshape shape: torch.Size([8, 160, 16, 16])
# After decoder2 shape: torch.Size([8, 128, 32, 32])
# After add t3 shape: torch.Size([8, 128, 32, 32])
# After dblock2 shape: torch.Size([8, 1024, 128])
# After dnorm4 and reshape shape: torch.Size([8, 128, 32, 32])
# After decoder3 shape: torch.Size([8, 32, 64, 64])
# After add t2 shape: torch.Size([8, 32, 64, 64])
# After decoder4 shape: torch.Size([8, 16, 128, 128])
# After add t1 shape: torch.Size([8, 16, 128, 128])
# After decoder5 shape: torch.Size([8, 16, 256, 256])

    def forward(self, x):

        print(f"Input shape: {x.shape}")
        B = x.shape[0]

        ### Encoder
        ### Conv Stage

        ### Stage 1
        out = F.relu(F.max_pool2d(self.encoder1(x), 2, 2))
        # print(f"After Stage 1 (encoder1) shape: {out.shape}")
        t1 = out
        t1 = self.cbam(t1)
        
        ### Stage 2
        out = F.relu(F.max_pool2d(self.encoder2(out), 2, 2))
        # print(f"After Stage 2 (encoder2) shape: {out.shape}")
        t2 = out
        t2 = self.cbam1(t2)

        ### Stage 3
        out = F.relu(F.max_pool2d(self.encoder3(out), 2, 2))
        # print(f"After Stage 3 (encoder3) shape: {out.shape}")
        t3 = out
        t3 = self.cbam2(t3)

        ### Tokenized KAN Stage
        ### Stage 4

        out, H, W = self.patch_embed3(out)
        # print(f"After Stage 4 (patch_embed3) shape: {out.shape}, H: {H}, W: {W}")
        for i, blk in enumerate(self.block1):
            out = blk(out, H, W)
        out = self.norm3(out)
        out = out.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()
        # print(f"After norm3 and reshape shape: {out.shape}")
        t4 = out

        ### Bottleneck

        out, H, W = self.patch_embed4(out)
        print(f"After Bottleneck (patch_embed4) shape: {out.shape}, H: {H}, W: {W}")
        for i, blk in enumerate(self.block2):
            out = blk(out, H, W)
        out = self.norm4(out)
        out = out.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()
        # print(f"After norm4 and reshape shape: {out.shape}")

        ### Decoder
        ### Stage 4
        out = F.relu(F.interpolate(self.decoder1(out), scale_factor=(2, 2), mode='bilinear'))
        # print(f"After decoder1 shape: {out.shape}")
        out = torch.add(out, t4)
        _, _, H, W = out.shape
        # print(f"After add t4 shape: {out.shape}")

        out = out.flatten(2).transpose(1, 2)
        for i, blk in enumerate(self.dblock1):
            out = blk(out, H, W)
        # print(f"After dblock1 shape: {out.shape}")

        ### Stage 3
        out = self.dnorm3(out)
        out = out.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()
        # print(f"After dnorm3 and reshape shape: {out.shape}")
        out = F.relu(F.interpolate(self.decoder2(out), scale_factor=(2, 2), mode='bilinear'))
        # print(f"After decoder2 shape: {out.shape}")
        out = torch.add(out, t3)
        # print(f"After add t3 shape: {out.shape}")
        _, _, H, W = out.shape
        out = out.flatten(2).transpose(1, 2)

        for i, blk in enumerate(self.dblock2):
            out = blk(out, H, W)
        # print(f"After dblock2 shape: {out.shape}")

        out = self.dnorm4(out)
        out = out.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()
        # print(f"After dnorm4 and reshape shape: {out.shape}")

        out = F.relu(F.interpolate(self.decoder3(out), scale_factor=(2, 2), mode='bilinear'))
        # print(f"After decoder3 shape: {out.shape}")
        out = torch.add(out, t2)
        # print(f"After add t2 shape: {out.shape}")
        out = F.relu(F.interpolate(self.decoder4(out), scale_factor=(2, 2), mode='bilinear'))
        # print(f"After decoder4 shape: {out.shape}")
        out = torch.add(out, t1)
        # print(f"After add t1 shape: {out.shape}")
        out = F.relu(F.interpolate(self.decoder5(out), scale_factor=(2, 2), mode='bilinear'))
        # print(f"After decoder5 shape: {out.shape}")

        return self.final(out)
            
#     def forward(self, x):
# #        x = self.cbam(x)
        
#         print(f"Input shape111111111111111111111111111: {x.shape}")
#         B = x.shape[0]
#         ### Encoder
#         ### Conv Stage

#         ### Stage 1
#         out = F.relu(F.max_pool2d(self.encoder1(x), 2, 2))
#         t1 = out
#         ### Stage 2
#         out = F.relu(F.max_pool2d(self.encoder2(out), 2, 2))
#         t2 = out
#         ### Stage 3
#         out = F.relu(F.max_pool2d(self.encoder3(out), 2, 2))
#         t3 = out

#         ### Tokenized KAN Stage
#         ### Stage 4

#         out, H, W = self.patch_embed3(out)
#         for i, blk in enumerate(self.block1):
#             out = blk(out, H, W)
#         out = self.norm3(out)
#         out = out.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()
#         t4 = out

#         ### Bottleneck

#         out, H, W= self.patch_embed4(out)
#         for i, blk in enumerate(self.block2):
#             out = blk(out, H, W)
#         out = self.norm4(out)
#         out = out.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()

#         ### Stage 4
#         out = F.relu(F.interpolate(self.decoder1(out), scale_factor=(2,2), mode ='bilinear'))

#         out = torch.add(out, t4)
#         _, _, H, W = out.shape
#         out = out.flatten(2).transpose(1,2)
#         for i, blk in enumerate(self.dblock1):
#             out = blk(out, H, W)

#         ### Stage 3
#         out = self.dnorm3(out)
#         out = out.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()
#         out = F.relu(F.interpolate(self.decoder2(out),scale_factor=(2,2),mode ='bilinear'))
#         out = torch.add(out,t3)
#         _,_,H,W = out.shape
#         out = out.flatten(2).transpose(1,2)
        
#         for i, blk in enumerate(self.dblock2):
#             out = blk(out, H, W)

#         out = self.dnorm4(out)
#         out = out.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()

#         out = F.relu(F.interpolate(self.decoder3(out),scale_factor=(2,2),mode ='bilinear'))
#         out = torch.add(out,t2)
#         out = F.relu(F.interpolate(self.decoder4(out),scale_factor=(2,2),mode ='bilinear'))
#         out = torch.add(out,t1)
#         out = F.relu(F.interpolate(self.decoder5(out),scale_factor=(2,2),mode ='bilinear'))

#         return self.final(out)

class KM_UNet(nn.Module):
    def __init__(self, num_classes, input_channels=3, deep_supervision=False, img_size=224, patch_size=16, in_chans=3, embed_dims=[256, 320, 512], no_kan=False,
    drop_rate=0., drop_path_rate=0., norm_layer=nn.LayerNorm, depths=[1, 1, 1], **kwargs):
        super().__init__()

        kan_input_dim = embed_dims[0]

        self.encoder1 = ConvLayer(3, kan_input_dim//8)  
        self.encoder2 = ConvLayer(kan_input_dim//8, kan_input_dim//4)  
        self.encoder3 = ConvLayer(kan_input_dim//4, kan_input_dim)

        self.norm3 = norm_layer(embed_dims[1])
        self.norm4 = norm_layer(embed_dims[2])

        self.dnorm3 = norm_layer(embed_dims[1])
        self.dnorm4 = norm_layer(embed_dims[0])

        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]

        self.block1 = nn.ModuleList([KANBlock(
            dim=embed_dims[1], 
            drop=drop_rate, drop_path=dpr[0], norm_layer=norm_layer
            )])

        self.block2 = nn.ModuleList([KANBlock(
            dim=embed_dims[2],
            drop=drop_rate, drop_path=dpr[1], norm_layer=norm_layer
            )])

        self.dblock1 = nn.ModuleList([KANBlock(
            dim=embed_dims[1], 
            drop=drop_rate, drop_path=dpr[0], norm_layer=norm_layer
            )])

        self.dblock2 = nn.ModuleList([KANBlock(
            dim=embed_dims[0], 
            drop=drop_rate, drop_path=dpr[1], norm_layer=norm_layer
            )])

        self.patch_embed3 = PatchEmbed(img_size=img_size // 4, patch_size=3, stride=2, in_chans=embed_dims[0], embed_dim=embed_dims[1])
        self.patch_embed4 = PatchEmbed(img_size=img_size // 8, patch_size=3, stride=2, in_chans=embed_dims[1], embed_dim=embed_dims[2])

        self.decoder1 = D_ConvLayer(embed_dims[2], embed_dims[1])  
        self.decoder2 = D_ConvLayer(embed_dims[1], embed_dims[0])  
        self.decoder3 = D_ConvLayer(embed_dims[0], embed_dims[0]//4) 
        self.decoder4 = D_ConvLayer(embed_dims[0]//4, embed_dims[0]//8)
        self.decoder5 = D_ConvLayer(embed_dims[0]//8, embed_dims[0]//8)

        self.final = nn.Conv2d(embed_dims[0]//8, num_classes, kernel_size=1)
        self.soft = nn.Softmax(dim =1)
        self.cbam = CBAM(channel=16)
        self.cbam1 = CBAM(channel=32)
        self.cbam2 = CBAM(channel=128)
        # SS2D模块
        self.ss2d_1 = SS2D(d_model=16)    # Stage 1后
        self.ss2d_2 = SS2D(d_model=32)    # Stage 2后
        self.ss2d_3 = SS2D(d_model=128)   # Stage 3后
        self.ss2d_decoder1 = SS2D(d_model=160)  # Decoder1后
        self.ss2d_decoder2 = SS2D(d_model=128)  # Decoder2后
        self.ss2d_decoder3 = SS2D(d_model=32)   # Decoder3后
        self.ss2d_decoder4 = SS2D(d_model=16)   # Decoder4后
        # EMA注意力机制
        self.ema1 = EMA(channels=16)
        self.ema2 = EMA(channels=32)
        self.ema3 = EMA(channels=128)
        self.ema_decoder1 = EMA(channels=160)
        self.ema_decoder2 = EMA(channels=128)
        self.ema_decoder3 = EMA(channels=32)
        self.ema_decoder4 = EMA(channels=16)


    def forward(self, x):

        print(f"Input shape: {x.shape}")
        B = x.shape[0]

        ### Encoder
        ### Conv Stage

        ### Stage 1
        out = F.relu(F.max_pool2d(self.encoder1(x), 2, 2))  # 输出通道数为16
        t1 = out
        # t1 = self.cbam(t1)
        t1 = t1.permute(0, 2, 3, 1).contiguous()  # 从BCHW -> BHWC
        t1 = self.ss2d_1(t1)  # SS2D的d_model设置为16
        t1 = t1.permute(0, 3, 1, 2).contiguous()  # 从BHWC -> BCHW
        t1 = self.ema1(t1)  # Apply EMA
        
        ### Stage 2
        out = F.relu(F.max_pool2d(self.encoder2(out), 2, 2))  # 输出通道数为32
        t2 = out
        # t2 = self.cbam1(t2)
        t2 = t2.permute(0, 2, 3, 1).contiguous()  # 从BCHW -> BHWC
        t2 = self.ss2d_2(t2)  # SS2D的d_model设置为32
        t2 = t2.permute(0, 3, 1, 2).contiguous()  # 从BHWC -> BCHW
        t2 = self.ema2(t2)  # Apply EMA
        
        ### Stage 3
        out = F.relu(F.max_pool2d(self.encoder3(out), 2, 2))  # 输出通道数为128
        t3 = out
        # t3 = self.cbam2(t3)
        t3 = t3.permute(0, 2, 3, 1).contiguous()  # 从BCHW -> BHWC
        t3 = self.ss2d_3(t3)  # SS2D的d_model设置为128
        t3 = t3.permute(0, 3, 1, 2).contiguous()  # 从BHWC -> BCHW
        t3 = self.ema3(t3)  # Apply EMA
        
        ### Tokenized KAN Stage
        ### Stage 4
        out, H, W = self.patch_embed3(out)  # 输出通道数为256
        for i, blk in enumerate(self.block1):
            out = blk(out, H, W)
        out = self.norm3(out)
        out = out.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()
        t4 = out

        ### Bottleneck
        out, H, W = self.patch_embed4(out)  # 输出通道数为64
        print(f"After Bottleneck (patch_embed4) shape: {out.shape}, H: {H}, W: {W}")
        for i, blk in enumerate(self.block2):
            out = blk(out, H, W)
        out = self.norm4(out)
        out = out.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()

        ### Decoder
        ### Stage 4
        out = F.relu(F.interpolate(self.decoder1(out), scale_factor=(2, 2), mode='bilinear'))  # 输出通道数为160
        out = torch.add(out, t4)
        out = out.permute(0, 2, 3, 1).contiguous()  # 从BCHW -> BHWC
        out = self.ss2d_decoder1(out)  # SS2D的d_model设置为160
        out = out.permute(0, 3, 1, 2).contiguous()  # 从BHWC -> BCHW
        out = self.ema_decoder1(out)  # Apply EMA
        
        ### Stage 3
        out = F.relu(F.interpolate(self.decoder2(out), scale_factor=(2, 2), mode='bilinear'))  # 输出通道数为128
        out = torch.add(out, t3)
        out = out.permute(0, 2, 3, 1).contiguous()  # 从BCHW -> BHWC
        out = self.ss2d_decoder2(out)  # SS2D的d_model设置为128
        out = out.permute(0, 3, 1, 2).contiguous()  # 从BHWC -> BCHW
        out = self.ema_decoder2(out)  # Apply EMA
        
        ### Stage 2
        out = F.relu(F.interpolate(self.decoder3(out), scale_factor=(2, 2), mode='bilinear'))  # 输出通道数为32
        out = torch.add(out, t2)
        out = out.permute(0, 2, 3, 1).contiguous()  # 从BCHW -> BHWC
        out = self.ss2d_decoder3(out)  # SS2D的d_model设置为32
        out = out.permute(0, 3, 1, 2).contiguous()  # 从BHWC -> BCHW
        out = self.ema_decoder3(out)  # Apply EMA
        
        ### Stage 1
        out = F.relu(F.interpolate(self.decoder4(out), scale_factor=(2, 2), mode='bilinear'))  # 输出通道数为16
        out = torch.add(out, t1)
        out = out.permute(0, 2, 3, 1).contiguous()  # 从BCHW -> BHWC
        out = self.ss2d_decoder4(out)  # SS2D的d_model设置为16
        out = out.permute(0, 3, 1, 2).contiguous()  # 从BHWC -> BCHW
        out = self.ema_decoder4(out)  # Apply EMA
        
        out = F.relu(F.interpolate(self.decoder5(out), scale_factor=(2, 2), mode='bilinear'))  # 输出通道数为16

        return self.final(out)


    
    
    
    
    
class UKANPP(nn.Module):  # UKANPP stands for U-Net++ with KAN
    def __init__(self, num_classes, input_channels=3, deep_supervision=False, img_size=224, patch_size=16, in_chans=3, embed_dims=[256, 320, 512], no_kan=False,
                 drop_rate=0., drop_path_rate=0., norm_layer=nn.LayerNorm, depths=[1, 1, 1], **kwargs):
        super().__init__()

        kan_input_dim = embed_dims[0]

        # Encoding layers (U-Net++)
        self.encoder1 = ConvLayer(3, kan_input_dim // 8)
        self.encoder2 = ConvLayer(kan_input_dim // 8, kan_input_dim // 4)
        self.encoder3 = ConvLayer(kan_input_dim // 4, kan_input_dim)

        # Normalization layers
        self.norm3 = norm_layer(embed_dims[1])
        self.norm4 = norm_layer(embed_dims[2])

        # D-Block normalization layers
        self.dnorm3 = norm_layer(embed_dims[1])
        self.dnorm4 = norm_layer(embed_dims[0])

        # DropPath initialization for stochastic depth
        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]

        # Tokenized KAN layers
        self.block1 = nn.ModuleList([KANBlock(dim=embed_dims[1], drop=drop_rate, drop_path=dpr[0], norm_layer=norm_layer)])
        self.block2 = nn.ModuleList([KANBlock(dim=embed_dims[2], drop=drop_rate, drop_path=dpr[1], norm_layer=norm_layer)])
        
        # Decoder blocks (for upsampling and U-Net++ style connections)
        self.decoder1 = D_ConvLayer(embed_dims[2], embed_dims[1])  # Upsample and connect with encoder 3 output
        self.decoder2 = D_ConvLayer(embed_dims[1], embed_dims[0])  # Upsample and connect with encoder 2 output
        self.decoder3 = D_ConvLayer(embed_dims[0], embed_dims[0] // 4)  # Upsample and connect with encoder 1 output
        self.decoder4 = D_ConvLayer(embed_dims[0] // 4, embed_dims[0] // 8)
        self.decoder5 = D_ConvLayer(embed_dims[0] // 8, embed_dims[0] // 8)

        # Patch Embedding layers for KAN integration
        self.patch_embed3 = PatchEmbed(img_size=img_size // 4, patch_size=3, stride=2, in_chans=embed_dims[0], embed_dim=embed_dims[1])
        self.patch_embed4 = PatchEmbed(img_size=img_size // 8, patch_size=3, stride=2, in_chans=embed_dims[1], embed_dim=embed_dims[2])

        # U-Net++ dense connections
        self.plusplus_conv2_1 = ConvLayer(embed_dims[1], embed_dims[0])
        self.plusplus_conv3_2 = ConvLayer(embed_dims[2], embed_dims[1])

        # Final output layer
        self.final = nn.Conv2d(embed_dims[0] // 8, num_classes, kernel_size=1)
        self.soft = nn.Softmax(dim=1)

    def forward(self, x):
        B = x.shape[0]

        # Encoder stages with dense connections
        out1 = F.relu(F.max_pool2d(self.encoder1(x), 2, 2))  # Encoder 1
        out2 = F.relu(F.max_pool2d(self.encoder2(out1), 2, 2))  # Encoder 2
        out3 = F.relu(F.max_pool2d(self.encoder3(out2), 2, 2))  # Encoder 3

        # Patch embedding and KAN blocks (tokenized KAN processing)
        out, H, W = self.patch_embed3(out3)
        for i, blk in enumerate(self.block1):
            out = blk(out, H, W)
        out = self.norm3(out).reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()

        out, H, W = self.patch_embed4(out)
        for i, blk in enumerate(self.block2):
            out = blk(out, H, W)
        out = self.norm4(out).reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()

        # U-Net++ style decoder with dense connections
        upsampled = F.relu(F.interpolate(self.decoder1(out), scale_factor=2, mode='bilinear'))
        upsampled = torch.add(upsampled, out3)  # Add encoder3 output

        upsampled = F.relu(F.interpolate(self.decoder2(upsampled), scale_factor=2, mode='bilinear'))
        upsampled = torch.add(upsampled, out2)  # Add encoder2 output

        # Add dense connections in U-Net++ manner
        out2_plus = F.relu(self.plusplus_conv2_1(out2))
        upsampled = torch.add(upsampled, out2_plus)

        upsampled = F.relu(F.interpolate(self.decoder3(upsampled), scale_factor=2, mode='bilinear'))
        upsampled = torch.add(upsampled, out1)  # Add encoder1 output

        out1_plus = F.relu(self.plusplus_conv3_2(out1))
        upsampled = torch.add(upsampled, out1_plus)

        # Final upsampling layers
        upsampled = F.relu(F.interpolate(self.decoder4(upsampled), scale_factor=2, mode='bilinear'))
        upsampled = F.relu(F.interpolate(self.decoder5(upsampled), scale_factor=2, mode='bilinear'))

        return self.final(upsampled)
    

class KMUNetPP(nn.Module):
    def __init__(self, num_classes, input_channels=3, deep_supervision=False, img_size=224, patch_size=16, in_chans=3, embed_dims=[128, 160, 256], no_kan=False, # Adjusted embed_dims
                 drop_rate=0., drop_path_rate=0., norm_layer=nn.LayerNorm, depths=[1, 1, 1], # Assuming depths match KAN blocks
                 ss2d_d_state=16, ss2d_expand=2, ema_factor=8, **kwargs):
        super().__init__()

        # Define channel dimensions
        enc1_ch = embed_dims[0] // 8 # e.g., 16
        enc2_ch = embed_dims[0] // 4 # e.g., 32
        enc3_ch = embed_dims[0]      # e.g., 128
        kan1_dim = embed_dims[1]     # e.g., 160
        kan2_dim = embed_dims[2]     # e.g., 256

        # --- Encoding layers ---
        self.encoder1 = ConvLayer(input_channels, enc1_ch)
        self.encoder2 = ConvLayer(enc1_ch, enc2_ch)
        self.encoder3 = ConvLayer(enc2_ch, enc3_ch)

        # --- SS2D Modules (applied to encoder outputs before use) ---
        self.ss2d_1 = SS2D(d_model=enc1_ch, d_state=ss2d_d_state, expand=ss2d_expand)
        self.ss2d_2 = SS2D(d_model=enc2_ch, d_state=ss2d_d_state, expand=ss2d_expand)
        self.ss2d_3 = SS2D(d_model=enc3_ch, d_state=ss2d_d_state, expand=ss2d_expand)
        # SS2D for decoder stages (applied after additions)
        self.ss2d_decoder1 = SS2D(d_model=kan1_dim, d_state=ss2d_d_state, expand=ss2d_expand)
        self.ss2d_decoder2 = SS2D(d_model=enc3_ch, d_state=ss2d_d_state, expand=ss2d_expand)
        self.ss2d_decoder3 = SS2D(d_model=enc2_ch, d_state=ss2d_d_state, expand=ss2d_expand)
        self.ss2d_decoder4 = SS2D(d_model=enc1_ch, d_state=ss2d_d_state, expand=ss2d_expand)

        # --- EMA Modules (applied after SS2D) ---
        self.ema1 = EMA(channels=enc1_ch, factor=ema_factor)
        self.ema2 = EMA(channels=enc2_ch, factor=ema_factor)
        self.ema3 = EMA(channels=enc3_ch, factor=ema_factor)
        self.ema_decoder1 = EMA(channels=kan1_dim, factor=ema_factor)
        self.ema_decoder2 = EMA(channels=enc3_ch, factor=ema_factor)
        self.ema_decoder3 = EMA(channels=enc2_ch, factor=ema_factor)
        self.ema_decoder4 = EMA(channels=enc1_ch, factor=ema_factor)

        # --- KAN related ---
        self.norm3 = norm_layer(kan1_dim) # KAN block 1 output
        self.norm4 = norm_layer(kan2_dim) # KAN block 2 output (bottleneck)
        # No decoder KAN blocks in this U-Net++ structure for simplicity,
        # can be added back if needed by adapting the forward pass.
        # self.dnorm3 = norm_layer(kan1_dim)
        # self.dnorm4 = norm_layer(enc3_ch)

        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]
        # KAN Blocks (Encoder Path)
        self.block1 = nn.ModuleList([KANBlock(dim=kan1_dim, drop=drop_rate, drop_path=dpr[0], norm_layer=norm_layer, no_kan=no_kan)])
        self.block2 = nn.ModuleList([KANBlock(dim=kan2_dim, drop=drop_rate, drop_path=dpr[1], norm_layer=norm_layer, no_kan=no_kan)])

        # Patch Embeddings
        self.patch_embed3 = PatchEmbed(img_size=img_size // 8, patch_size=3, stride=2, in_chans=enc3_ch, embed_dim=kan1_dim)
        self.patch_embed4 = PatchEmbed(img_size=img_size // 16, patch_size=3, stride=2, in_chans=kan1_dim, embed_dim=kan2_dim)

        # --- Decoder Convolutions ---
        self.decoder1 = D_ConvLayer(kan2_dim, kan1_dim)
        self.decoder2 = D_ConvLayer(kan1_dim, enc3_ch)
        self.decoder3 = D_ConvLayer(enc3_ch, enc2_ch)
        self.decoder4 = D_ConvLayer(enc2_ch, enc1_ch)
        self.decoder5 = D_ConvLayer(enc1_ch, enc1_ch)

        # --- U-Net++ style dense connection processing layers ---
        # Process the SS2D/EMA'd encoder outputs before adding them in the decoder
        self.plusplus_conv_enc2 = ConvLayer(enc2_ch, enc2_ch) # Process t2 (processed enc2)
        self.plusplus_conv_enc1 = ConvLayer(enc1_ch, enc1_ch) # Process t1 (processed enc1)

        # --- Final output layer ---
        self.final = nn.Conv2d(enc1_ch, num_classes, kernel_size=1)
        self.soft = nn.Softmax(dim=1)


    def forward(self, x):
        B = x.shape[0]
        # print(f"KMUNetPP Input shape: {x.shape}")

        # --- Encoder stages with SS2D/EMA processing ---
        # Stage 1
        enc1 = self.encoder1(x)
        out1 = F.max_pool2d(enc1, 2, 2) # X_0,0 raw
        out1_ss2d = self.ss2d_1(out1.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)
        t1 = self.ema1(out1_ss2d) # Processed X_0,0 (skip connection)
        # print(f"KMUNetPP t1 (Processed X_0,0): {t1.shape}")

        # Stage 2
        enc2 = self.encoder2(t1) # Input is processed t1
        out2 = F.max_pool2d(enc2, 2, 2) # X_1,0 raw
        out2_ss2d = self.ss2d_2(out2.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)
        t2 = self.ema2(out2_ss2d) # Processed X_1,0 (skip connection)
        # print(f"KMUNetPP t2 (Processed X_1,0): {t2.shape}")

        # Stage 3
        enc3 = self.encoder3(t2) # Input is processed t2
        out3 = F.max_pool2d(enc3, 2, 2) # X_2,0 raw
        out3_ss2d = self.ss2d_3(out3.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)
        t3 = self.ema3(out3_ss2d) # Processed X_2,0 (skip connection)
        # print(f"KMUNetPP t3 (Processed X_2,0): {t3.shape}")

        # --- Tokenized KAN Stage ---
        # Stage 4 (Input is processed t3)
        kan1_in, H1, W1 = self.patch_embed3(t3)
        kan1_out = self.block1[0](kan1_in, H1, W1)
        kan1_out = self.norm3(kan1_out).reshape(B, H1, W1, -1).permute(0, 3, 1, 2).contiguous() # X_3,0 : KAN block 1 output
        t4 = kan1_out # Use KAN output as a skip connection
        # print(f"KMUNetPP t4 (X_3,0): {t4.shape}")

        # Bottleneck (Input is KAN block 1 output t4)
        kan2_in, H2, W2 = self.patch_embed4(t4)
        bottleneck_out = self.block2[0](kan2_in, H2, W2)
        bottleneck_out = self.norm4(bottleneck_out).reshape(B, H2, W2, -1).permute(0, 3, 1, 2).contiguous() # X_4,0 : Bottleneck KAN output
        # print(f"KMUNetPP bottleneck (X_4,0): {bottleneck_out.shape}")

        # --- U-Net++ style Decoder with SS2D/EMA and dense connections ---
        # Level 1 Decode (from bottleneck X_4,0)
        dec1_up = F.relu(F.interpolate(self.decoder1(bottleneck_out), scale_factor=2, mode='bilinear')) # Upsample X_4,0
        dec1_add = torch.add(dec1_up, t4) # Add X_3,0 (KAN skip conn)
        # Apply SS2D/EMA after addition
        dec1_ss2d = self.ss2d_decoder1(dec1_add.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)
        dec1_out = self.ema_decoder1(dec1_ss2d) # Output of Decoder Level 1
        # print(f"KMUNetPP dec1_out: {dec1_out.shape}")

        # Level 2 Decode (from dec1_out)
        dec2_up = F.relu(F.interpolate(self.decoder2(dec1_out), scale_factor=2, mode='bilinear'))
        dec2_add = torch.add(dec2_up, t3) # Add X_2,0 (processed Enc3 skip conn)
        # Apply SS2D/EMA after addition
        dec2_ss2d = self.ss2d_decoder2(dec2_add.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)
        dec2_out = self.ema_decoder2(dec2_ss2d) # Output of Decoder Level 2
        # print(f"KMUNetPP dec2_out: {dec2_out.shape}")

        # Level 3 Decode (from dec2_out)
        dec3_up = F.relu(F.interpolate(self.decoder3(dec2_out), scale_factor=2, mode='bilinear'))
        dec3_add = torch.add(dec3_up, t2) # Add X_1,0 (processed Enc2 skip conn)
        # Add U-Net++ dense connection (processed t2)
        t2_processed_dense = F.relu(self.plusplus_conv_enc2(t2))
        dec3_add_dense = torch.add(dec3_add, t2_processed_dense)
        # Apply SS2D/EMA after all additions
        dec3_ss2d = self.ss2d_decoder3(dec3_add_dense.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)
        dec3_out = self.ema_decoder3(dec3_ss2d) # Output of Decoder Level 3
        # print(f"KMUNetPP dec3_out: {dec3_out.shape}")

        # Level 4 Decode (from dec3_out)
        dec4_up = F.relu(F.interpolate(self.decoder4(dec3_out), scale_factor=2, mode='bilinear'))
        dec4_add = torch.add(dec4_up, t1) # Add X_0,0 (processed Enc1 skip conn)
        # Add U-Net++ dense connection (processed t1)
        t1_processed_dense = F.relu(self.plusplus_conv_enc1(t1))
        dec4_add_dense = torch.add(dec4_add, t1_processed_dense)
        # Apply SS2D/EMA after all additions
        dec4_ss2d = self.ss2d_decoder4(dec4_add_dense.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)
        dec4_out = self.ema_decoder4(dec4_ss2d) # Output of Decoder Level 4
        # print(f"KMUNetPP dec4_out: {dec4_out.shape}")

        # --- Final Upsampling layers ---
        dec5_out = F.relu(F.interpolate(self.decoder5(dec4_out), scale_factor=2, mode='bilinear'))
        final_up = F.interpolate(dec5_out, scale_factor=(2, 2), mode='bilinear')
        # print(f"KMUNetPP final_up: {final_up.shape}")

        return self.final(final_up)



